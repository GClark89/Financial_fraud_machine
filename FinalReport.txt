To conclude this project, we should answer the following 5 questions in a separate document attached to your project: 
i. Which insights did you gain from your EDA? 

One of the most notable findings is the significant class imbalance, with 99.9% of transactions being non-fraudulent and only 0.1% being fraudulent. 
This imbalance makes detecting fraud a real challenge. We also noticed that most fraudulent transactions tend to be of the TRANSFER and CASH_OUT types, 
while PAYMENT and CASH_IN are generally non-fraudulent. Additionally, the IsFlaggedFraud feature doesn't seem very effective, as many fraudulent transactions, 
particularly CASH_OUT and TRANSFER, are not flagged at all. Larger transaction amounts were found to be more likely to be fraudulent, though they aren't always flagged.


There were also some outliers in the TRANSFER and CASH_OUT categories, where large transactions were more often linked to fraudulent activity. 
These findings suggest that the current fraud detection model needs some fine-tuning, especially in identifying high-risk TRANSFER and CASH_OUT transactions. 
To improve detection, it might help to focus on features like transaction amounts or timing patterns, and consider more advanced "unique" detection or 
machine learning approaches for spotting these rare, high-risk events.


ii. How did you determine which columns to drop or keep? If your EDA informed this process, explain which insights you used to determine which columns were not needed. 

I removed the columns that seemed redundant initally. So that allowed me to get rid of 'nameDest'& 'nameOrig'. IsFlaggedFraud was dropped because there was not 
enough data in the coulumn. Only transaction type that was flagged was Transfers and that was certaintly not the only type that was actually fraud. 
Removing step was a tough decision but looking at my scatter plot i can see a lot of payment types happening at a cetain interval which surely could have possibly helped point 
out what transactions were fraud , with the class imbalance, i felt it wouldn't help much. i Think after running my model, i would like to use step just to compare the difference, 
if there is any. 


iii. What was your final F1 Score? 

The final f1 score was .60. This indicates that the logistic regression model managed to strike a decent balance between precision and recall. 
In the context of fraud detection, it means the model was able to identify some fraudulent transactions, but it still made some errors, 
like false positives or false negatives. While an F1 score of 1.0 would be ideal, 0.60 is a reasonable starting point, especially considering how tough it can 
be to detect fraud accurately with imbalanced data.

